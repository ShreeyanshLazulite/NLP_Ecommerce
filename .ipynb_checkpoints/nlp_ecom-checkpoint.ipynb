{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2fd3ad",
   "metadata": {},
   "source": [
    "# NLP Ecommerce Text Classification\n",
    "### By Shreeyansh\n",
    "#### shreeyanshparihar@gmail.com | +971 529412388 | +91 9530056916\n",
    "\n",
    "Prelimnary Step: Install required Python Libary. In case only transformaer is need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bc0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\yadav\\appdata\\roaming\\python\\python39\\site-packages (4.22.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\yadav\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\yadav\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ff77b",
   "metadata": {},
   "source": [
    "### Importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230ec141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe032d",
   "metadata": {},
   "source": [
    "### Create a batch tokenizer to encode tags in data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1445f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_tokenizer(tokenizer, dataset):\n",
    "    return tokenizer.batch_encode_plus(dataset,\n",
    "                                       max_length=256,\n",
    "                                       padding=True,\n",
    "                                       truncation=True,\n",
    "                                       add_special_tokens=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3f8ee",
   "metadata": {},
   "source": [
    "### Process tokenized dataset and input generate to final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7624b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val\n",
    "                in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb73759",
   "metadata": {},
   "source": [
    "### Method to compute precision, accuracy etc. after training the model and testing with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9394b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    prediction, labels = p\n",
    "    preds_flat = np.argmax(prediction, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b154b",
   "metadata": {},
   "source": [
    "### Load data from CSV and convert to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91da4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ecommerceDataset.csv\", names=[\"labels\", \"descriptions\"])\n",
    "descriptions = df[\"descriptions\"].map(str).values.tolist()\n",
    "labels = df[\"labels\"].values.tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef949c",
   "metadata": {},
   "source": [
    "### Load BERT Sequence Classification pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eadda05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f549",
   "metadata": {},
   "source": [
    "### Load BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0abf62be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9168ccb6fad46dfa9403969e57d7263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42da8670c7914a0b97a38ace4561d709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c374de",
   "metadata": {},
   "source": [
    "### Split the dataset into training, validation and test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4103c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(descriptions, labels, test_size=0.4, stratify=labels, random_state=42)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d58e7",
   "metadata": {},
   "source": [
    "### Tokenize the tags in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c3f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = get_batch_tokenizer(tokenizer, x_train)\n",
    "x_valid_tokens = get_batch_tokenizer(tokenizer, x_valid)\n",
    "x_test_tokens = get_batch_tokenizer(tokenizer, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b69cd",
   "metadata": {},
   "source": [
    "### Preparing Final Dataset with the tokenised labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e59a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(x_train_tokens, y_train)\n",
    "valid_dataset = Dataset(x_valid_tokens, y_valid)\n",
    "test_dataset = Dataset(x_test_tokens, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269199f",
   "metadata": {},
   "source": [
    "### Preparing Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "486f9066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(output_dir=\"output\",\n",
    "                            evaluation_strategy=\"epoch\",\n",
    "                            metric_for_best_model=\"f1\",\n",
    "                            save_strategy=\"epoch\",\n",
    "                            num_train_epochs=3,\n",
    "                            load_best_model_at_end=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a5a22",
   "metadata": {},
   "source": [
    "### Preparing the Trainer Object & Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e3ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args=args,\n",
    "                    model=model,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=valid_dataset,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(\n",
    "                            early_stopping_patience=3)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2649052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yadav\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30255\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11346\n",
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_13256\\3846330469.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11346' max='11346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11346/11346 23:36:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.200522</td>\n",
       "      <td>0.965516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.146698</td>\n",
       "      <td>0.975050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.133567</td>\n",
       "      <td>0.977441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10085\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-3782\n",
      "Configuration saved in output\\checkpoint-3782\\config.json\n",
      "Model weights saved in output\\checkpoint-3782\\pytorch_model.bin\n",
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_13256\\3846330469.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10085\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-7564\n",
      "Configuration saved in output\\checkpoint-7564\\config.json\n",
      "Model weights saved in output\\checkpoint-7564\\pytorch_model.bin\n",
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_13256\\3846330469.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10085\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output\\checkpoint-11346\n",
      "Configuration saved in output\\checkpoint-11346\\config.json\n",
      "Model weights saved in output\\checkpoint-11346\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output\\checkpoint-11346 (score: 0.9774410552863159).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11346, training_loss=0.13990063367225328, metrics={'train_runtime': 85003.9155, 'train_samples_per_second': 1.068, 'train_steps_per_second': 0.133, 'total_flos': 1.194085189020672e+16, 'train_loss': 0.13990063367225328, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e6b9d",
   "metadata": {},
   "source": [
    "### Test the trained model with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c8b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10085\n",
      "  Batch size = 8\n",
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_13256\\3846330469.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123d4db",
   "metadata": {},
   "source": [
    "### Get the predicted value and true values, then prepare a classificaton report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f39afab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=1).flatten()\n",
    "true_vals = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecd5c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Books       0.99      0.97      0.98      2335\n",
      "Clothing & Accessories       0.98      0.99      0.99      1772\n",
      "           Electronics       0.98      0.97      0.97      2111\n",
      "             Household       0.98      0.99      0.98      3867\n",
      "\n",
      "              accuracy                           0.98     10085\n",
      "             macro avg       0.98      0.98      0.98     10085\n",
      "          weighted avg       0.98      0.98      0.98     10085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_vals, preds, target_names=list(le.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b752857",
   "metadata": {},
   "source": [
    "### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e76157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Books' 'Clothing & Accessories' 'Electronics' 'Household']\n",
      "[3 3 0 ... 0 2 2]\n",
      "[3 3 0 ... 0 2 2]\n",
      "Healthgenie Water Bed (Colour May Vary) Healthgenie Water Bed ensure long life and comfort. It is made up of single textured rubberised fabric that is skin friendly and its variable firmness allows it to adjust according to your body which makes it perfect. It not only gives aesthetic pleasure but is also a complete solution to back ache, spinal problem, burns, bedsores, arthritis, general surgery, cardiac rehabilitation, cystic fibrosis, cerebral palsy and multiple sclerosis. It helps you sleep in your natural body position and foster better circulation. It comes in standard size that can be easily placed on your bed. Based on the water therapy it is known to relax both body and mind. Further it comes in various colours to add grace to your living room. It is easy to clean and maintain. And most importantly it is leak proof. Features, single textured rubberised fabric. Completely leak proof, therapeutic, durable, suitable for all ages, neutralizes the pressure points in your body, ensure sound sleep.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "print(true_vals)\n",
    "print(preds)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a87dfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test = ['The iPhone 14 and iPhone 14 Plus will be available for pre-order starting today and will cost $799 and $899, respectively. The iPhone 14 will be more widely available on September 16th, while the iPhone 14 Plus will hit stores on October 7th.This year, the Pro phones have a noticeably different design than previous iPhones. The rumors about a pill-shaped cutout turned out to be true â€” the screen notch is now gone and has been replaced by a floating space that houses the front-facing cameras as well as Apple\\'s privacy dots, which turn on when apps use your camera or microphone. From a software standpoint, that space is dubbed the \"Dynamic Island\" as it will change and expand to adapt to what you\\'re doing on your iPhone, notifications you receive and more.The iPhone 14 Pro has a 6.1-inch display while the Pro Max has a 6.7-inch screen, and they\\'re always-on for the first time ever. Apple designed the panel to be as power efficient as possible, dynamically adjusting the refresh rate down to as low as 1Hz when necessary. The new Lock Screen in iOS 16 can show a bunch of things on the display like the time, widgets, live activities and more, and the Pro screens will do things like automatically dim to preserve power while continuing to show you relevant information, Lock Screen photos and backgrounds and more.As expected, the Pro handsets run on Apple\\'s new A16 Bionic chip and they have an updated rear camera array along with a new TrueDepth front-facing camera. The rear setup includes a new 12MP telephoto lens, a 12MP ultra wide camera and a 48-megapixel main shooter that has a 65-percent larger sensor than that in the iPhone 13 Pro. The Pro phones will also support all of the new features found on the standard iPhone 14 models, including 5G and eSIM connectivity, crash detection, Emergency SOS with Satellite and more.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ed7631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test_token = get_batch_tokenizer(tokenizer,manual_test)\n",
    "manual_test_dataset = Dataset(manual_test_token,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de2a69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_13256\\3846330469.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n"
     ]
    }
   ],
   "source": [
    "manual_pred = trainer.predict(manual_test_dataset)\n",
    "manual_test_prediction = np.argmax(manual_pred.predictions, axis=1).flatten()\n",
    "print(le.classes_[manual_test_prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bedf82c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_model\n",
      "Configuration saved in trained_model\\config.json\n",
      "Model weights saved in trained_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a114825",
   "metadata": {},
   "source": [
    "## Program Ends Here \n",
    "### NLP Ecommerce Text Classification\n",
    "#### By Shreeyansh\n",
    "##### shreeyanshparihar@gmail.com | +971 529412388 | +91 9530056916"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
